import os
import json
import argparse
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader
from torchvision import datasets, transforms as T

from models.registry import build_model, load_class_names

# ---------- CONFIG DEFAULTS ----------
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]
DATA_ROOT = "data/carambola/processed"
WEIGHTS_DIR = "weights"
IMG_SIZE = 224
NUM_WORKERS = 2
PIN_MEMORY = True
# -------------------------------------

def build_dataloaders(data_root, img_size, batch_size):
    train_tf = T.Compose([
        T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
        T.RandomHorizontalFlip(p=0.5),
        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        T.ToTensor(),
        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),
    ])
    test_tf = T.Compose([
        T.Resize((img_size, img_size)),
        T.ToTensor(),
        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),
    ])

    train_ds = datasets.ImageFolder(os.path.join(data_root, "train"), transform=train_tf)
    val_ds   = datasets.ImageFolder(os.path.join(data_root, "val"),   transform=test_tf)
    test_ds  = datasets.ImageFolder(os.path.join(data_root, "test"),  transform=test_tf)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,
                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)
    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,
                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)
    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,
                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

    return train_loader, val_loader, test_loader, train_ds.classes

def accuracy_top1(logits, targets):
    preds = torch.argmax(logits, dim=1)
    correct = (preds == targets).sum().item()
    return correct / targets.size(0)

def evaluate(model, loader, device, loss_fn):
    model.eval()
    total_loss, total_acc, total_n = 0.0, 0.0, 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
            out = model(x)
            loss = loss_fn(out, y)
            acc = accuracy_top1(out, y)

            b = y.size(0)
            total_loss += loss.item() * b
            total_acc  += acc * b
            total_n    += b
    return total_loss / total_n, total_acc / total_n

def train_one_epoch(model, loader, device, loss_fn, optimizer, scaler=None):
    model.train()
    total_loss, total_acc, total_n = 0.0, 0.0, 0
    for x, y in loader:
        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)
        if scaler is not None:
            with torch.cuda.amp.autocast():
                out = model(x)
                loss = loss_fn(out, y)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            out = model(x)
            loss = loss_fn(out, y)
            loss.backward()
            optimizer.step()

        acc = accuracy_top1(out, y)
        b = y.size(0)
        total_loss += loss.item() * b
        total_acc  += acc * b
        total_n    += b

    return total_loss / total_n, total_acc / total_n

def save_checkpoint(path, model, class_names, arch, img_size, epoch, val_acc, extra=None):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    payload = {
        "state_dict": model.state_dict(),
        "arch": arch,
        "class_names": class_names,
        "img_size": img_size,
        "mean": IMAGENET_MEAN,
        "std": IMAGENET_STD,
        "epoch": epoch,
        "val_acc": float(val_acc),
        "saved_at": datetime.now().isoformat(timespec="seconds"),
    }
    if extra:
        payload.update(extra)
    torch.save(payload, path)

def main(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")

    # class names (generated by your split script)
    class_names = load_class_names(os.path.join(WEIGHTS_DIR, "class_names.json"))
    num_classes = len(class_names)
    print(f"Classes ({num_classes}): {class_names}")

    # model
    model, _ = build_model(args.arch, num_classes=num_classes, pretrained=args.pretrained)
    model.to(device)

    # Optionally freeze backbone for transfer learning (resnet18 path)
    if args.arch == "resnet18" and args.freeze_backbone:
        for name, p in model.named_parameters():
            # freeze everything except final classifier (fc) layer
            if not name.startswith("fc."):
                p.requires_grad = False
        print("Backbone frozen; training classifier head only.")

    # data
    train_loader, val_loader, test_loader, discovered_classes = build_dataloaders(
        DATA_ROOT, args.img_size, args.batch_size
    )
    # Ensure discovered order matches class_names.json (good to log)
    if discovered_classes != class_names:
        print("WARNING: Class order in dataset differs from class_names.json.")
        print("Dataset classes: ", discovered_classes)

    # loss / opt / sched
    loss_fn = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                            lr=args.lr, weight_decay=args.weight_decay)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)

    scaler = torch.cuda.amp.GradScaler(enabled=(device == "cuda"))

    best_val_acc = 0.0
    best_epoch = -1
    early_stop_counter = 0

    for epoch in range(1, args.epochs + 1):
        train_loss, train_acc = train_one_epoch(model, train_loader, device, loss_fn, optimizer, scaler)
        val_loss, val_acc = evaluate(model, val_loader, device, loss_fn)
        scheduler.step()

        print(f"Epoch {epoch:03d}/{args.epochs} | "
              f"train_loss={train_loss:.4f} acc={train_acc:.4f} | "
              f"val_loss={val_loss:.4f} acc={val_acc:.4f}")

        # save best
        if val_acc > best_val_acc + 1e-4:
            best_val_acc = val_acc
            best_epoch = epoch
            ckpt_name = f"{args.arch}_best.pt" if not args.ckpt_name else args.ckpt_name
            ckpt_path = os.path.join(WEIGHTS_DIR, ckpt_name)
            save_checkpoint(ckpt_path, model, class_names, args.arch, args.img_size, epoch, val_acc,
                            extra={"train_acc": float(train_acc)})
            print(f"  âœ… Saved new best: {ckpt_path} (val_acc={val_acc:.4f})")
            early_stop_counter = 0
        else:
            early_stop_counter += 1

        if args.early_stop_patience > 0 and early_stop_counter >= args.early_stop_patience:
            print(f"Early stopping at epoch {epoch}. Best val acc: {best_val_acc:.4f} (epoch {best_epoch})")
            break

    # Evaluate best checkpoint on test set (if saved)
    ckpt_name = f"{args.arch}_best.pt" if not args.ckpt_name else args.ckpt_name
    ckpt_path = os.path.join(WEIGHTS_DIR, ckpt_name)
    if os.path.exists(ckpt_path):
        print(f"\nLoading best checkpoint for test evaluation: {ckpt_path}")
        payload = torch.load(ckpt_path, map_location="cpu")
        model.load_state_dict(payload["state_dict"], strict=False)
        model.to(device)
        test_loss, test_acc = evaluate(model, test_loader, device, loss_fn)
        print(f"TEST  | loss={test_loss:.4f} acc={test_acc:.4f}")
    else:
        print("\nNo best checkpoint found; skipping test evaluation.")

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--arch", type=str, default="resnet18",
                   choices=["custom_cnn", "s_cnn", "resnet18"],
                   help="Which model to train")
    p.add_argument("--pretrained", action="store_true",
                   help="Use pretrained backbone where available (e.g., ResNet18)")
    p.add_argument("--freeze_backbone", action="store_true",
                   help="Freeze all layers except final classifier (transfer learning warmup)")
    p.add_argument("--img_size", type=int, default=IMG_SIZE)
    p.add_argument("--batch_size", type=int, default=32)
    p.add_argument("--epochs", type=int, default=15)
    p.add_argument("--lr", type=float, default=1e-3)
    p.add_argument("--weight_decay", type=float, default=1e-4)
    p.add_argument("--label_smoothing", type=float, default=0.0)
    p.add_argument("--early_stop_patience", type=int, default=5,
                   help="Stop if val acc doesn't improve after N epochs (0=disabled)")
    p.add_argument("--ckpt_name", type=str, default="",
                   help="Optional custom filename for saved checkpoint")
    args = p.parse_args()
    main(args)
